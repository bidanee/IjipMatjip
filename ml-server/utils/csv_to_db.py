# import os
# import pandas as pd
# import psycopg2
# from dotenv import load_dotenv
# import requests
# import time

# def get_coords(address, api_key):
#   """
#   ì¹´ì¹´ì˜¤ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
#   """
#   url = "https://dapi.kakao.com/v2/local/search/address.json"
#   headers = {"Authorization": f"KakaoAK {api_key}"}
#   params = {"query": address}
#   try:
#     response = requests.get(url, headers=headers, params=params).json()
#     if response['documents']:
#       location = response['documents'][0]
#       return float(location['y']), float(location['x'])
#   except Exception as e:
#     print(f"ì£¼ì†Œ ë³€í™˜ ì‹¤íŒ¨: {address}, ì˜¤ë¥˜: {e}")
#   return None, None

# def migrate_csv_to_db():
#   """
#     csv íŒŒì¼ì„ ì½ì–´ PostgreSQL DBì— í…Œì´ë¸”ì„ ë§Œë“¤ê³  ë°ì´í„° ì‚½ì…
#   """
#   load_dotenv()
#   conn = None
#   try:
#     # 1. ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
#     conn = psycopg2.connect(
#       host=os.getenv("DB_HOST"),
#       port=os.getenv("DB_PORT"),
#       dbname=os.getenv("DB_NAME"),
#       user=os.getenv("DB_USER"),
#       password=os.getenv("DB_PASSWORD")
#     )
#     cur = conn.cursor()
#     print("ë°ì´í„° ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
    
#     # 2. í…Œì´ë¸” ìƒì„± SQL (transactions í…Œì´ë¸” ì¶”ê°€)
#     create_tables_sql = """
#     DROP TABLE IF EXISTS schools, subways, neighborhood_scores, transactions;
    
#     CREATE TABLE schools (
#       id SERIAL PRIMARY KEY,
#       name VARCHAR(255),
#       latitude DOUBLE PRECISION,
#       longitude DOUBLE PRECISION,
#       geom GEOMETRY(Point, 4326)
#     );
    
#     CREATE TABLE subways (
#       id SERIAL PRIMARY KEY,
#       name VARCHAR(255),
#       latitude DOUBLE PRECISION,
#       longitude DOUBLE PRECISION,
#       geom GEOMETRY(Point, 4326)
#     );
    
#     CREATE TABLE neighborhood_scores(
#       id SERIAL PRIMARY KEY,
#       dong VARCHAR(255),
#       sigungu_name VARCHAR(255),
#       school_count INTEGER,
#       subway_count INTEGER,
#       avg_price BIGINT,
#       school_score FLOAT,
#       subway_score FLOAT,
#       price_score FLOAT,
#       latitude DOUBLE PRECISION,
#       longitude DOUBLE PRECISION
#     );
    
#     CREATE TABLE transactions(
#       id SERIAL PRIMARY KEY,
#       sigungu VARCHAR(255),
#       dong_name VARCHAR(255),
#       complex_name VARCHAR(255),
#       area FLOAT,
#       price INTEGER,
#       floor INTEGER,
#       build_year INTEGER,
#       latitude DOUBLE PRECISION,
#       longitude DOUBLE PRECISION
#     );
#     """
#     cur.execute(create_tables_sql)
#     print("í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
#     # 3. CSV ë°ì´í„° DBì— ì‚½ì…
#     print("CSV íŒŒì¼ì„ DBì— ì‚½ì… ì¤‘...")
    
#     # í•™êµ ë°ì´í„°
#     school_df = pd.read_csv('../datas/encoding_schools.csv', encoding='utf-8-sig')
#     for index, row in school_df.iterrows():
#       cur.execute(
#         "INSERT INTO schools (name, latitude, longitude, geom) VALUES (%s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326))",(row['í•™êµëª…'],row['ìœ„ë„'], row['ê²½ë„'], row['ê²½ë„'], row['ìœ„ë„'])
#       )
#     print("í•™êµ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
    
#     # ì§€í•˜ì² ì—­ ë°ì´í„°
#     subway_df = pd.read_csv('../datas/subway_combined.csv', encoding='utf-8-sig')
#     for index, row in subway_df.iterrows():
#       cur.execute(
#         "INSERT INTO subways (name, latitude, longitude, geom) VALUES (%s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326))",(row['ì—­ëª…'],row['ìœ„ë„'], row['ê²½ë„'], row['ê²½ë„'], row['ìœ„ë„'])
#       )    
#     print("ì§€í•˜ì² ì—­ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
    
#     # ë™ë„¤ ì ìˆ˜ ë°ì´í„°
#     scores_df = pd.read_csv('../datas/neighborhood_final_scores.csv')
#     for index, row in scores_df.iterrows():
#       cur.execute(
#         """
#         INSERT INTO neighborhood_scores (dong, sigungu_name, school_count, subway_count, avg_price, school_score, subway_score, price_score, latitude, longitude) 
#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#         """,
#         (row['dong'], row['sigungu_name'], row['school_count'], row['subway_count'], row['avg_price'], row['school_score'], row['subway_score'], row['price_score'], row.get('latitude'), row.get('longitude'))
#       )
#     print("ë™ë„¤ ì ìˆ˜ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
    
#     # --- ê±°ë˜ ë°ì´í„° ì‚½ì… ë¡œì§ ---
#     trade_df = pd.read_csv('../datas/seoul_transactions_202306.csv', encoding='utf-8-sig')
    
#     # ì‹œêµ°êµ¬ì—ì„œ 'dong_name'ì¶”ì¶œ
#     trade_df['dong_name'] = trade_df['ì‹œêµ°êµ¬'].str.split().str[2]
    
#     # 'ê±°ë˜ê¸ˆì•¡(ë§Œì›)' ì»¬ëŸ¼ì—ì„œ ì‰¼í‘œ ì œê±° í›„ ìˆ«ìë¡œ ë³€í™˜
#     trade_df['ê±°ë˜ê¸ˆì•¡(ë§Œì›)'] = pd.to_numeric(trade_df['ê±°ë˜ê¸ˆì•¡(ë§Œì›)'].str.replace(',',''), errors='coerce')
    
#     trade_df.dropna(subset=['ê±°ë˜ê¸ˆì•¡(ë§Œì›)','ë„ë¡œëª…'], inplace=True)
    
#     api_key = os.getenv("KAKAO_REST_API_KEY")
#     for index, row in trade_df.iterrows():
#       full_address = f"{row['ì‹œêµ°êµ¬']} {row.get('ë„ë¡œëª…','')}"
#       lat, lng = get_coords(full_address, api_key)
#       if lat and lng:
#         cur.execute(
#           """
#           INSERT INTO transactions (sigungu, dong_name, complex_name, area, price, floor, build_year, latitude, longitude)
#           VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
#           """,
#           (row['ì‹œêµ°êµ¬'],row['dong_name'], row['ë‹¨ì§€ëª…'], row['ì „ìš©ë©´ì (ã¡)'], row['ê±°ë˜ê¸ˆì•¡(ë§Œì›)'], row['ì¸µ'], row['ê±´ì¶•ë…„ë„'], lat, lng)
#         )
#       time.sleep(0.05)
#     print("-> ê±°ë˜ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
    
#     # 4. ê³µê°„ ì¸ë±ìŠ¤ ìƒì„± (ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ì†ë„ í–¥ìƒ)
#     create_index_sql = """
#     CREATE INDEX schools_geom_idx ON schools USING GIST (geom);
#     CREATE INDEX subways_geom_idx ON subways USING GIST (geom);
#     """
#     cur.execute(create_index_sql)
#     print("ê³µê°„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
#     # ë³€ê²½ì‚¬í•­ ì €ì¥
#     conn.commit()
#     print("ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
#   except Exception as e:
#     if conn:
#       conn.rollback()
#     print(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#   finally:
#     if conn:
#       conn.close()
#       print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ ")
      
# if __name__ == '__main__':
#   migrate_csv_to_db()


import os
import pandas as pd
import psycopg2
from dotenv import load_dotenv

def migrate_csv_to_db():
    load_dotenv()
    conn = None
    try:
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST"), port=os.getenv("DB_PORT"),
            dbname=os.getenv("DB_NAME"), user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )
        cur = conn.cursor()
        print("âœ… DB ì—°ê²° ì„±ê³µ")

        # í…Œì´ë¸” ìƒì„± SQL
        create_tables_sql = """
        DROP TABLE IF EXISTS schools, subways;
        CREATE TABLE schools (
            id SERIAL PRIMARY KEY, name VARCHAR(255),
            latitude DOUBLE PRECISION, longitude DOUBLE PRECISION,
            geom GEOGRAPHY(Point, 4326)
        );
        CREATE TABLE subways (
            id SERIAL PRIMARY KEY, name VARCHAR(255),
            latitude DOUBLE PRECISION, longitude DOUBLE PRECISION,
            geom GEOGRAPHY(Point, 4326)
        );
        """
        cur.execute(create_tables_sql)
        print("âœ… schools, subways í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

        print("CSV ë°ì´í„° ì‚½ì…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # í•™êµ ë°ì´í„° (INSERT ë¬¸ ìˆ˜ì •)
        school_df = pd.read_csv('../datas/encoding_schools.csv', encoding='utf-8-sig')
        # ìœ„ë„, ê²½ë„ ê°’ì´ ì—†ëŠ” ë°ì´í„° ì œê±°
        school_df.dropna(subset=['ìœ„ë„', 'ê²½ë„'], inplace=True)
        for index, row in school_df.iterrows():
            # ğŸ‘‡ ST_SetSRID(ST_MakePoint(ê²½ë„, ìœ„ë„), 4326) ë¥¼ ì‚¬ìš©í•´ geom ì»¬ëŸ¼ ì±„ìš°ê¸°
            cur.execute(
                "INSERT INTO schools (name, latitude, longitude, geom) VALUES (%s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326))",
                (row['í•™êµëª…'], row['ìœ„ë„'], row['ê²½ë„'], row['ê²½ë„'], row['ìœ„ë„'])
            )
        print("-> í•™êµ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
        
        # ì§€í•˜ì²  ë°ì´í„° (INSERT ë¬¸ ìˆ˜ì •)
        subway_df = pd.read_csv('../datas/subway_combined.csv', encoding='utf-8-sig')
        subway_df.dropna(subset=['ìœ„ë„', 'ê²½ë„'], inplace=True)
        for index, row in subway_df.iterrows():
            cur.execute(
                "INSERT INTO subways (name, latitude, longitude, geom) VALUES (%s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326))",
                (row['ì—­ëª…'], row['ìœ„ë„'], row['ê²½ë„'], row['ê²½ë„'], row['ìœ„ë„'])
            )
        print("-> ì§€í•˜ì²  ë°ì´í„° ì‚½ì… ì™„ë£Œ")
        
        # ê³µê°„ ì¸ë±ìŠ¤ ìƒì„±
        create_index_sql = """
        CREATE INDEX schools_geom_idx ON schools USING GIST (geom);
        CREATE INDEX subways_geom_idx ON subways USING GIST (geom);
        """
        cur.execute(create_index_sql)
        print("âœ… ê³µê°„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

        conn.commit()
        print("ğŸ‰ ì¸í”„ë¼ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        if conn: conn.rollback()
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if conn: conn.close()
        print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ")

if __name__ == '__main__':
    # ì°¸ê³ : ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ì œ ì¸í”„ë¼ ë°ì´í„°ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    migrate_csv_to_db()